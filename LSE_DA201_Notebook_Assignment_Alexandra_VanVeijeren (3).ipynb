{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84f07f93",
   "metadata": {},
   "source": [
    "### LSE Data Analytics Online Career Accelerator\n",
    "\n",
    "# DA201: Data Analytics using Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6a7d7b",
   "metadata": {},
   "source": [
    "## Assignment template: COVID-19 data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9157b71c-1db1-4122-8975-627aa7d1b1a5",
   "metadata": {},
   "source": [
    "## Student Note\n",
    "This template can be used to better understand the suggested assignment workflow and how to approach the questions. You are welcome to add code and Markdown blocks to the various sections to add either code or comments. Make sure to add code cells as applicable, and to comment all your code blocks.\n",
    "\n",
    "You have the option to populate your Notebook with all the elements typically contained within the report, or to submit a separate report. In the case of submitting your Notebook, you can embed images, links and text where appropriate in addition to the text notes added.\n",
    "\n",
    "**SPECIAL NOTE**\n",
    "- Submit your Jupyter Notebook with the following naming convention: `LSE_DA201_assignment_[your name]_[your surname]` (remove the square brackets).\n",
    "- You should submit a zipped folder containing all the elements used in your Notebook (data files, images, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a895e5d",
   "metadata": {},
   "source": [
    "> ***Markdown notes:*** Remember to change cell types to `Markdown`. You can review [Markdown basics](https://docs.github.com/en/get-started/writing-on-github/getting-started-with-writing-and-formatting-on-github/basic-writing-and-formatting-syntax) to find out how to add formatted text, links and images to your Markdown documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f155d2-e4d0-423d-aa87-b8240fcadbfe",
   "metadata": {},
   "source": [
    "## 0) Environment preparation\n",
    "These settings are provided for you. You do not need to make any changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10f4193-c2a2-4fdc-95b3-c04385631ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries and set the plotting options\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set(rc = {'figure.figsize':(15,10)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416654dd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1) Assignment activity 1: \n",
    "\n",
    "### 1.1) Report/notebook expectations:\n",
    "- Illustrate your GitHub setup consisting of the load and push updates of all the Jupyter Notebook files. (**Hint**: Make sure that your GitHub username, the directory structure and updates are visible in the screenshot. Make sure to provide a zipped copy of the folder containing your submission Notebook as well as supporting files such as images used in the Notebook.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a261c768-dff6-425b-b2e0-aa5b47e5d2fa",
   "metadata": {},
   "source": [
    "### Required: Report submission:\n",
    "Insert URL (to your public GitHub repository) and a screenshot - double click cell to edit\n",
    "- [My Github Repo](https://github.com/username/reponame)\n",
    "- Screenshot demo (replace with your own).\n",
    "\n",
    "!['My Github screenshot](http://github.com/apvoges/lse-ca/blob/main/GitHubScreenshot.png?raw=true)\n",
    "(Note that this only works if your repo is set to **public**. Alternatively you need to refer to a local image and include this image in your submission.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5430b3ce-3d82-4fd7-9163-a9094c7d0632",
   "metadata": {},
   "source": [
    "### 1.2) Presentation expectations:\n",
    "- Describe the role and how workflow tools such as GitHub can be used to add value to organisations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3848ff2a-a0c8-4807-bf1d-c8e40f3b5f10",
   "metadata": {},
   "source": [
    "### Optional for notebook/Required for presentation.\n",
    "- You can use this cell as placeholder for bullet points to include in your presentation. \n",
    "- This section will not be graded in the Notebook, grades awarded based on presentation content only. \n",
    "\n",
    "(Double-click to edit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a44f1af-fa0d-4d5b-92fa-4e6896958644",
   "metadata": {},
   "source": [
    "## 2) Assignment activity 2: \n",
    "\n",
    "### 2.1) Report expectations:\n",
    "- Load the files `covid_19_uk_cases.csv` and `covid_19_uk_vaccinated.csv` and explore the data.\n",
    "- Explore the data using the `info()`, `describe()`, `shape` and `value_counts()` methods, and note the observations regarding data types, number or records and features.\n",
    "- Identify missing data.\n",
    "- Filter/subset data.\n",
    "- Aggregate data (totals and by month).\n",
    "- Note observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18199fa-8e5e-42e6-9a19-b8766730a16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the COVID-19 cases and vaccine data sets as cov and vac respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd43318-2f54-4fd6-a4f8-b392c2430284",
   "metadata": {},
   "outputs": [],
   "source": [
    "cov = pd.read_csv('covid_19_uk_cases.csv')\n",
    "vac = pd.read_csv('covid_19_uk_vaccinated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c58b44-7f82-4862-99a8-89345187c84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the DataFrames with the appropriate functions\n",
    "# sense check the DataFrames\n",
    "cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869e9113",
   "metadata": {},
   "outputs": [],
   "source": [
    "vac"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f96551",
   "metadata": {},
   "source": [
    "## Sense check the data\n",
    "To get a better idea of the data, I opened the csvs in excel so that I would be able to check if the correct number of rows and columns imported for each data set. I used the shape and columns functions to see this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5fd0d9-987d-4e50-b0d9-6966d13cd467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the number of rows and columns of the DataFrames to sense check the data.\n",
    "print(cov.shape)\n",
    "print(vac.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3a9e9c-fc2b-4d16-8389-3bdb56639d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the column names included in each data set.\n",
    "print(cov.columns)\n",
    "print(vac.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e63fe06",
   "metadata": {},
   "source": [
    "## Initial exploration of the data\n",
    "In this step, I determine the data types of each column in each data set and identify the number of missing values in the data sets. I still need to decide how to treat the missing values and if the data types of each column are acceptable for analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8584e15-0a30-4d79-8087-63085620c518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the data types of each column in the cov data sets. \n",
    "print(cov.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1636b26d-8524-4b09-b20e-a3e05d07643d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the data types of each column in the cov data sets.\n",
    "print(vac.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce0c304",
   "metadata": {},
   "source": [
    "#### Objects are categorical variables.\n",
    "This implies that every object which is not an integer or float is a categorical variable. The only column for which this might need to be changed is the 'Date' column. Below, I view the date columns to get a sense of whether the data was recorded continuously over time or on distinct reporting days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca8491e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the date column\n",
    "cov['Date']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea851ef",
   "metadata": {},
   "source": [
    "Dates are recorded continuously with a period of one day.I suspect that the data is classified as categorical because for each region, there will be an entry for that day. The number of covid cases on January 1st 2020 for each region can be compared. In order to study this data overtime, dates must be converted to datetime data types. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa782f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the date column to the correct object\n",
    "cov['Date'] = pd.to_datetime(cov['Date'])\n",
    "vac['Date'] = pd.to_datetime(vac['Date'])\n",
    "\n",
    "# Sense check the data\n",
    "print(\"Date type for vac data: \", vac.dtypes['Date'])\n",
    "print(\"Date type for vac data: \", cov.dtypes['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e7f17f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Determine the number of missing values in the cov data set.\n",
    "cov.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76d131f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the number of missing values in the vac data set.\n",
    "vac.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a857d2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the rows containing missing values\n",
    "cov[cov.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d80589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward fill the missing values\n",
    "cov = cov.fillna(method='ffill')\n",
    "\n",
    "# Sense check to ensure the missing values have been imputed\n",
    "cov[cov.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db853f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the previously missing values\n",
    "cov.iloc[875:877]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71366d8",
   "metadata": {},
   "source": [
    "It can be seen that only the cov data set has missing values. These are in the 'deaths', 'cases', 'recovered' and 'hospitalised' columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8b9b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the index of the datasets to 'Date to enable time series analysis'\n",
    "cov = cov.set_index('Date')\n",
    "vac = vac.set_index('Date')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb859f8",
   "metadata": {},
   "source": [
    "# Explore the Distribution of the data as a whole"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33d6573",
   "metadata": {},
   "source": [
    "The data is first normalised, to enable comparison between datasets at a later stage. I will use maximum absolute scaling to determine to scale data points between values of -1 and 1. I hope to identify if the distributions of the two subsets differ. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79782f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define maximum absolute scaling as a function\n",
    "def max_abs_scaling(df): #define a function that uses a df as input\n",
    "    #copy the DataFrame\n",
    "    df_scaled = df.copy()\n",
    "    if isinstance(df_scaled, pd.DataFrame):    #checks if the df given is a df\n",
    "        #apply maximum absolute scaling\n",
    "        for column in df_scaled.columns:   #divide the value by the abs(max(value of the col))\n",
    "            df_scaled[column] = df_scaled[column] / df_scaled[column].abs().max()\n",
    "            \n",
    "    else:\n",
    "        df_scaled = df_scaled / df_scaled.abs().max()   #if not a dataframe, still norm. \n",
    "    \n",
    "    return df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affd0afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the distribution of the data as a whole\n",
    "\n",
    "# Keep only the relevant columns for comparison\n",
    "cases_study = cov[['Cases','Hospitalised', 'Recovered', 'Deaths']]\n",
    "\n",
    "# Normalise the data\n",
    "normal_cases = max_abs_scaling(cases_study)\n",
    "\n",
    "# View the distribution of the whole data\n",
    "sns.boxplot(data=normal_cases).set(title='The Distribution of Covid Data (normalised)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad34ec1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data appropriately\n",
    "cov_1 = cov.groupby(['Province/State'])[['Cases','Recovered', 'Hospitalised', 'Deaths']].sum()\n",
    "\n",
    "# Normalise the data to make comparable\n",
    "cov_1 = max_abs_scaling(cov_1)\n",
    "\n",
    "cov_1 = cov_1.reset_index()\n",
    "\n",
    "# Create a pairplot to identify outliers in the Covid data.\n",
    "sns.pairplot(data=cov_1, hue = 'Province/State')\n",
    "\n",
    "plt.ticklabel_format(style='Plain')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97335eba",
   "metadata": {},
   "source": [
    "'Others' is an outlying value in every category. It should there be removed from the Covid data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d007c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data appropriately\n",
    "vac_1 = vac.groupby(['Province/State'])[['Vaccinated','First Dose','Second Dose']].sum()\n",
    "\n",
    "# Normalise the data to make comparable\n",
    "vac_1 = max_abs_scaling(vac_1)\n",
    "\n",
    "vac_1 = vac_1.reset_index()\n",
    "\n",
    "# Create a pairplot to identify outliers in the Covid data.\n",
    "sns.pairplot(data=vac_1, hue = 'Province/State')\n",
    "\n",
    "plt.ticklabel_format(style='Plain')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e196820a",
   "metadata": {},
   "source": [
    "'Others' does not seem to be have outlying values in the vaccine data, but Gibraltar sits as the maximum value on each graph. Since the data has been normalised and aggregated, that implies that Gibraltar had the highest number of partially and fully vaccinated people per province. Gibraltar will be explored further to understand if this is accurate or erroneous.\n",
    "\n",
    "'Others' will be removed from the DataFrame that combines both the vaccine data and the covid data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808f17b3",
   "metadata": {},
   "source": [
    "## Explore the Gibraltar data\n",
    "The below code can be used to subset and explore each region included in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6b08f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the provinces included in the data sets \n",
    "print(cov['Province/State'].unique())\n",
    "\n",
    "print(vac['Province/State'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2aec1d0",
   "metadata": {},
   "source": [
    "It is expected that each province would have similar distributions of cases. To identify data which is could be erroneous, the distributions of each province are displayed using boxplots. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfed6fcb-8cf1-4bcf-9aff-836807f07ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame based on Gibraltar data\n",
    "# Hint: newdf = df[df[col]==index]\n",
    "gibraltar_cases = cov[cov['Province/State']=='Gibraltar'][['Cases','Hospitalised', 'Recovered', 'Deaths']]\n",
    "\n",
    "gibraltar_vac = vac[vac['Province/State']=='Gibraltar'][['Vaccinated', 'First Dose', 'Second Dose']]\n",
    "\n",
    "# print the whole DataFrame\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "\n",
    "# view the DataFrames to sense check\n",
    "print(gibraltar_cases.shape)\n",
    "print(gibraltar_vac.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02731bc4-d90b-46d7-9020-ebed51252509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore behaviour over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74eb2f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare Gibraltar data to Anguilla, and to the data as a whole \n",
    "\n",
    "anguilla_cases = cov[cov['Province/State']=='Anguilla'][['Cases','Hospitalised', 'Recovered', 'Deaths']]\n",
    "\n",
    "anguilla_vac = vac[vac['Province/State']=='Anguilla'][['Vaccinated', 'First Dose', 'Second Dose']]\n",
    "\n",
    "# print the whole DataFrame\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "\n",
    "# view the shape of the DataFrames to sense check\n",
    "print(anguilla_cases.shape)\n",
    "print(anguilla_vac.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc490b1",
   "metadata": {},
   "source": [
    "Comparing the two subsets is not entirely useful as their population sizes are different. I have chosen to normalise the Anguilla and the Gibraltar subsets to understand if their distributions are similar. I will use maximum absolute scaling to determine to scale data points between values of -1 and 1. I hope to identify if the distributions of the two subsets differ.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890789f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply maximum absolute feature scaling to the Anguilla data\n",
    "normal_anguilla_cases = max_abs_scaling(anguilla_cases)\n",
    "\n",
    "# view descriptive statistics for normalised data\n",
    "normal_anguilla_cases.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57877607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the same for Gibraltar\n",
    "normal_gibraltar_cases = max_abs_scaling(gibraltar_cases)\n",
    "\n",
    "# view descriptive statistics for normalised data\n",
    "normal_gibraltar_cases.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba0e1ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a9e0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the distribution of the Gibraltar data\n",
    "sns.boxplot(data=normal_gibraltar_cases).set(title = 'The distribution of Gibraltar data (normalised)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a34a9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the distribution of the Anguilla data\n",
    "sns.boxplot(data=normal_anguilla_cases).set(title = 'The distribution of Anguilla data (normalised)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa7acca",
   "metadata": {},
   "source": [
    "Gibraltar's range is much higher than Anguilla's. This indicates that there could be a problem with how Gibraltar's data was collected. Since this data is normalised, I would expect each province to have similar ranges. Gibraltar's data suggests that it has much higher numbers, in categories which differ from Anguilla's (for example, the Deaths and Hospitalisation columns). Further, Gibraltar's maximum number of deaths exceeds it's maximum number of cases. This would mean that more people died from Covid-19 in Gibraltar than the highest number of daily cases recorded. This is unreasonable and implies that something is wrong with the Gibraltar data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768f9e0d",
   "metadata": {},
   "source": [
    "Anguilla mirrors the distribution of the data as a whole. There are many outliers in the whole data set, as well as in Anguilla's distribution. The range of Gibraltar's values in each category is much higher than both the total dataset and the Anguilla. I will now remove Gibraltar from the dataset and check the distribution again, to see if the data has less outlying high values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90fc85c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create a dataframe without Gibraltar. \n",
    "cases_clean = cov[cov['Province/State'] != 'Gibraltar'][['Province/State','Cases','Hospitalised', 'Recovered', 'Deaths']].copy()\n",
    "\n",
    "# Keep only the relevant columns for comparison\n",
    "cases_clean_study = cases_clean[['Cases','Hospitalised', 'Recovered', 'Deaths']]\n",
    "\n",
    "#normalise\n",
    "cases_clean_normal = max_abs_scaling(cases_clean_study)\n",
    "\n",
    "# View the distribution of the whole data\n",
    "sns.boxplot(data=cases_clean_normal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741b2db3",
   "metadata": {},
   "source": [
    "The data's distribution stays fairly constant after removing the Gibraltar dataset. \n",
    "\n",
    "However, since there were errors in the collection of Gibraltar's Covid dataset, Gibraltar is removed from both the vaccine and Covid datasets to keep the number of provinces studied consistent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19ab661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Gibraltar from vaccine and the covid datasets. \n",
    "cov_clean = cov[cov['Province/State']!='Gibraltar']\n",
    "vac_clean = vac[vac['Province/State']!='Gibraltar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b547ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sense check Covid data\n",
    "cov_clean.loc[cov_clean['Province/State'] == 'Gibraltar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec59b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sense check vaccine data\n",
    "vac_clean.loc[vac_clean['Province/State'] == 'Gibraltar']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87b8d73",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0fc4f244-8514-4337-b581-9a25611b36b7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.2) Presentation expectations:\n",
    "Use the process of exploring the data for Gibraltar as an example to provide a brief description of the various phases to help your team to understand the process. Keep it high level and make sure to focus on both specifics relating to the case (first dose, second dose per region, total and over time) and brief observations regarding the process. Assignment activity 2 considers basic data exploration.\n",
    "- Can we make decisions based on total numbers only, or do trends over time offer additional insights?\n",
    "- Why it is important to explore the data, what are the typical mistakes made in this phase?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe004db6-d347-4eb7-a8bf-fc47a50e2aac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "861d8313-d8cf-4ed4-a885-fd0fd69375b5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3) Assignment activity 3: \n",
    "\n",
    "### 3.1) Report expectations:\n",
    "- Merge and explore the data.\n",
    "- Convert the data type of the Date column from object to DateTime.\n",
    "- Create a data set that meets the expected parameters.\n",
    "- Add calculated features to DataFrames (difference between first and second dose vaccinations).\n",
    "- Filter and sort output.\n",
    "- Observe totals and percentages as a total and over time.\n",
    "- Note observations.\n",
    "\n",
    "Merge the DataFrames without duplicating columns. The new DataFrame (e.g. `covid`) will have `7584` rows and the following columns: `Province/State, Country/Region, Date, Vaccinated, First Dose, Second Dose, Deaths, Cases, Recovered, Hospitalised`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fb4938",
   "metadata": {},
   "source": [
    "## The type of join used\n",
    "An inner join contains overlapping data in each dataset based on the join columns. The resulting dataframe must contain the correct data for the date and Province, and considering the data matches date for date, an inner join is appropriate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a02345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the DataFrames as covid where you merge cov and vac\n",
    "covid = pd.merge(cov_clean, vac_clean, on=['Province/State','Date'], how='inner')\n",
    "\n",
    "#covid.head()\n",
    "print(covid.columns)\n",
    "print(covid.loc[covid['Province/State'] == 'Gibraltar'])\n",
    "\n",
    "covid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a0cbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove 'Others' from the covid DataFrame\n",
    "covid = covid[covid['Province/State'] != 'Others']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a2d0fa-5af5-4897-ac2c-8fb4d3120377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the new DataFrame\n",
    "print(covid.shape)\n",
    "\n",
    "# identify the column names and column data types\n",
    "print(covid.columns)\n",
    "print(covid.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89a4105",
   "metadata": {},
   "source": [
    "The columns Country/Region, Lat, Long, ISO 3166-1 Alpha 3-Codes, Sub-region Name and Intermediate Region Code are duplicated and can be dropped from the covid DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0d972e-d5f8-4141-a046-9e55041382e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up / drop unnecessary columns \n",
    "dict1 = {'Province/State': 'Province/State', 'Country/Region_x':'Country/Region', 'Lat_x':'Lat',\n",
    "         'Long_y':'Long','ISO 3166-1 Alpha 3-Codes_x' : 'ISO 3166-1 Alpha 3-Codes',\n",
    "         'Sub-region Name_x':'Sub-region Name', \n",
    "         'Intermediate Region Code_x':'Intermediate Region Code', 'Date':'Date', \n",
    "        'Cases':'Cases', 'Recovered':'Recovered', 'Hospitalised':'Hospitalised', \n",
    "         'Vaccinated':'Vaccinated', 'First Dose':'First Dose', 'Second Dose': 'Second Dose'}\n",
    "\n",
    "covid.rename(columns=dict1, inplace=True)\n",
    "\n",
    "print(covid.columns)\n",
    "covid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dceee9b-21cc-47fd-bd8f-4ccc3cd6770d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groupby and calculate difference between first and second dose\n",
    "covid['Difference_Doses'] = covid['First Dose'] - covid['Second Dose']\n",
    "\n",
    "covid.groupby('Province/State')[['Difference_Doses']].sum().sort_values('Difference_Doses')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c6137e",
   "metadata": {},
   "source": [
    "### Observations\n",
    "All provinces have a positive difference between first and second doses. This means that more people across provinces had the first dose than the second dose. We see that Montserrat had the biggest difference between people who had the first dose and those who had the second. Saint Helena had the lowest difference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941ecc7f-2891-472f-90f5-d00bea73898b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index to groupby month\n",
    "covid = covid.reset_index()\n",
    "\n",
    "covid['Month']=covid['Date'].dt.to_period('M')\n",
    "vac_monthly = covid.groupby(['Province/State','Month'])[['Difference_Doses']].sum()\n",
    "vac_monthly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe61bb5d",
   "metadata": {},
   "source": [
    "It can be seen that for across the provinces, the monthly difference between those who have had their first dose and those who have had their second dose becomes negative in April and continues to be negative until October. This shows that after March, more people got their second dose than their first. \n",
    "The difference is at its lowest value in April and slowly increases towards October.  \n",
    "In October of 2021, more people got their first dose than their second dose . \n",
    "This implies that in April, a lot of people were getting their second dose. Slowly more people started to get their first dose while less received their second."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f252a92c-53a2-4c04-a639-6b360716506a",
   "metadata": {},
   "source": [
    "### 3.2) Presentation expectations:\n",
    "We use similar calculations and representations as we had in assignment activity 2, but now expand to look at all provinces. Assignment activity 3 is concerned with exploring data in the context of a specific business question (as opposed to general exploration in assignment activity 2).\n",
    "- What insights can be gained from the data? (Description of all regions, assumptions and concerns, trends or patterns you have observed.)\n",
    "- Are there limitations or assumptions that needs to be considered?\n",
    "- Make sure to provide a brief overview of the data and typical considerations at this phase of analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8320e7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which province has the highest number of partially vaccinated individuals? \n",
    "vaxed_totals = covid.groupby(['Province/State', 'Month'])[['Vaccinated','First Dose', 'Second Dose', 'Difference_Doses']].sum()\n",
    "\n",
    "print(vaxed_totals.loc[vaxed_totals['First Dose'].idxmax()])\n",
    "print()\n",
    "print(vaxed_totals.loc[vaxed_totals['First Dose'].idxmin()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9613fee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which province has the highest percentage of partially vaccinated individuals overall? \n",
    "\n",
    "# Calculate the monthly percentage of vaccinated individuals \n",
    "vaxed_totals['Partially Vaccinated'] = (vaxed_totals['First Dose'] / (vaxed_totals['First Dose'] + vaxed_totals['Vaccinated']) ) * 100\n",
    "\n",
    "vaxed_totals.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b711f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the city with the highest % of partially vaccinated people\n",
    "vaxed_totals.loc[pd.IndexSlice[:, '2021-09', :]].idxmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92fccc3-ec93-4096-a90a-6c864dadbdb5",
   "metadata": {},
   "source": [
    "## 4) Assignment activity 4: \n",
    "\n",
    "The government is looking to promote second dose vaccinations and would like to know the best possible area to test a new campaign. They are looking for the highest number of people who have received a first dose and not a second dose. \n",
    "- Where should they target?\n",
    "- Which provinces have the highest number (actual numbers) and highest relative numbers (second dose only/first dose)?\n",
    "- Visualise both outputs.\n",
    "\n",
    "### 4.1) Report expectations:\n",
    "- Consider additional features (deaths and recoveries).\n",
    "- Visualise the data.\n",
    "- Note observations:\n",
    " - Do deaths follow the same patterns observed in vaccination data (daily vs cumulative)?\n",
    " - Do we need to separate groups of data for specific variables and analyse them in isolation (Others) to be able to observe the patterns?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df7d7da-322b-47f4-a4aa-857835310948",
   "metadata": {},
   "source": [
    "### 4.2) Presentation expectations:\n",
    "- What insights can be gained from the data?\n",
    "- Why do we need to consider other features?\n",
    "- **Hints**: \n",
    " - Evalute different features to improve decision making (deaths and recoveries). \n",
    " - Why it is important to explore data and use different views?\n",
    " - Highlight two or three suggestions to get junior team members started in terms of good practices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24769ac-0135-45d3-8902-52765d523e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Absolute numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7bd40a-4afb-4bb1-8d15-6cadcd6fa335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate difference between first and second dose\n",
    "covid['Difference Doses'] = covid['First Dose'] - covid['Second Dose']\n",
    "\n",
    "# Calculate the total difference by province\n",
    "doses_by_province = covid.groupby('Province/State')[\n",
    "                    ['First Dose', 'Second Dose','Difference Doses']].sum().sort_values('Difference Doses')\n",
    "\n",
    "# View the data\n",
    "doses_by_province"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa2e92e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Calculate the ratio of interest\n",
    "doses_by_province['Eligible (%)'] = (doses_by_province['Difference Doses'] / \n",
    "                                 doses_by_province['First Dose']) * 100\n",
    "\n",
    "# Calculate the ratio of people who have received first dose to second\n",
    "doses_by_province['First : Second'] = (doses_by_province['First Dose'] / \n",
    "                                 doses_by_province['Second Dose'])\n",
    "\n",
    "doses_by_province = doses_by_province.reset_index()\n",
    "print(doses_by_province.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c196c67a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.ticklabel_format(style='plain')\n",
    "\n",
    "sns.barplot(x = 'First : Second', y='Province/State', data=doses_by_province)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76747afa",
   "metadata": {},
   "source": [
    "All recorded provinces have the same ratio of people who have received their first dose to those that have received their second. A better approach would be to see if the number of people going to get their second dose is increasing or decreasing over time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7fbd9d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualise the ratio of total first doses to total second doses\n",
    "doses_by_province[['First Dose', 'Second Dose']].plot(kind='bar', stacked=True).set_xticklabels(doses_by_province['Province/State'])\n",
    "# ax.ticklabel_format(style='plain')\n",
    "plt.title(\"Comparison between First Dose and Second Dose by Province\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812ed525",
   "metadata": {},
   "source": [
    "Here we can see that the ratio of aggregated fully vaccinated individuals is roughly equal to the number of people who received their first dose. Once again, a better indicator would be to consider the trend in vaccination. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fba2fd8-e5a5-439a-842c-620111120f83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Communicate the number of deaths per month per region\n",
    "fatalities = covid.groupby(['Province/State', 'Month'])[['Deaths']].sum()\n",
    "fatalities = fatalities.reset_index(['Province/State','Month'])\n",
    "\n",
    "# Reset the Month column data type to DateTime\n",
    "fatalities['Month']=pd.PeriodIndex(fatalities['Month'], freq='M').to_timestamp()\n",
    "\n",
    "# Sense check the data types\n",
    "print(fatalities.dtypes)\n",
    "\n",
    "fatalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99be12e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc = {'figure.figsize':(15,8)})\n",
    "sns.lineplot(x='Month', y='Deaths', hue='Province/State', data=fatalities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77d9c17",
   "metadata": {},
   "source": [
    "We see that in all provinces, the deaths per month are decreasing. The straight line part of each graph indicates that the  numbers of deaths are decreasing at a constant rate. This indicates that a peak in the daily deaths has been reached across most provinces. \n",
    "The Channel Islands has the highest number of monthly deaths out of all provinces. From our above vaccine data, they have an average rate of vaccination. Monteserrat has the highest number of partially and fully vaccinated people and one of the lowest deaths per month lines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebbca19-842b-4c90-88fb-3f96df93ce85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Communicate the number of recoveries per region\n",
    "recoveries = covid.groupby(['Province/State'])[['Recovered']].sum()\n",
    "recoveries = recoveries.reset_index()\n",
    "recoveries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88f0114-f7a3-44d1-b8af-15142d36bd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise recoveries in total\n",
    "fig, ax = plt.subplots()\n",
    "ax.ticklabel_format(style='plain')\n",
    "#ax.set_xticklabels( 'Province/State', rotation=90)\n",
    "\n",
    "sns.barplot(x = 'Recovered', y='Province/State', data=recoveries)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450c00b4",
   "metadata": {},
   "source": [
    "Observations:\n",
    "The Channel Islands had the largest number of recoveries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cc0efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is this consistent with the number of cases recorded? \n",
    "cases = covid.groupby(['Province/State'])[['Cases','Recovered', 'Hospitalised', 'Deaths']].sum()\n",
    "cases = cases.reset_index()\n",
    "sns.pairplot(data=cases, hue = 'Province/State', x_vars=['Cases', 'Deaths', 'Hospitalised'],\n",
    "            y_vars=['Recovered'])\n",
    "plt.ticklabel_format(style='Plain')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cb3ec3",
   "metadata": {},
   "source": [
    "This pairplot shows that the Channel Islands has the highest total deaths, hostpitalisations, cases and recoveries out of all of the Provinces. Most noticeably, the Channel Islands had the largest number of deaths for quite a low number of hospitalizations. The Channel Islands was not shown to be an outlier in my initial exploration and cleaning. I will consider the data as not erroneous, but rather note that the Channel Islands may be a good target for the vaccination campaign. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b939726-f592-4bc7-ad79-b0c2396523d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is this consistent overtime? \n",
    "recoveries_month = covid.groupby(['Province/State', 'Month'])[['Recovered']].sum()\n",
    "recoveries_month = recoveries_month.reset_index(['Province/State','Month'])\n",
    "\n",
    "# Reset the Month column data type to DateTime\n",
    "recoveries_month['Month']=pd.PeriodIndex(recoveries_month['Month'], freq='M').to_timestamp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96940a59-817c-4c26-8914-2e545d52c742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise recoveries overtime\n",
    "sns.set(rc = {'figure.figsize':(15,8)})\n",
    "sns.lineplot(x='Month', y='Recovered', hue='Province/State', data=recoveries_month)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4416a3d1",
   "metadata": {},
   "source": [
    "The Channel Islands has consistently had the highest recovery rate over the period of the data. From the previous graph, it is known that the Channel Islands was an outlying Province. It's data does follow the same trends as the other Provinces. There is a clear peak in July of 2021, and after that a steep decline in recoveries. This coincides with an increase in cases starting a bit before July 2021 (case graph seen below). The Channel Islands remains a good target for the marketing campaign.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8870ddd4-bf13-45b9-ade8-395a5cc8d7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other features evaluated (data preparation, output and plots)\n",
    "# Rate of Second Dose vaccination over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2945f94-ba7c-4adc-a117-039c97a37b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "doses_overtime = covid.groupby(['Province/State', 'Month'])[['First Dose', 'Second Dose' , 'Vaccinated']].sum()\n",
    "doses_overtime = doses_overtime.reset_index(['Province/State','Month'])\n",
    "\n",
    "# Reset the Month column data type to DateTime\n",
    "doses_overtime['Month']=pd.PeriodIndex(recoveries_month['Month'], freq='M').to_timestamp()\n",
    "\n",
    "doses_overtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2c9581-640b-4504-9326-183f7ee6d998",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualise second doses overtime overtime\n",
    "sns.set(rc = {'figure.figsize':(15,8)})\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.ticklabel_format(style='plain')\n",
    "\n",
    "sns.lineplot(x='Month', y = 'Second Dose', hue='Province/State', data=doses_overtime).set(title='Second Dose Overtime')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d2776a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise first dose overtime\n",
    "fig, ax = plt.subplots()\n",
    "ax.ticklabel_format(style='plain')\n",
    "\n",
    "g = sns.lineplot(x='Month', y='First Dose', hue='Province/State', data=doses_overtime)\n",
    "g.set(title='First Doses Overtime')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59f7f67",
   "metadata": {},
   "source": [
    "Across all provinces, there is a trend in the uptake of vaccinations. Between January and April of 2021, the number of people getting their first dose starts to plateau and then begins to decrease. The number of people getting their second dose starts to increase at the point where first doses start to decrease. This makes logical sense, as more people who had their first dose will be able to move onto getting their dose. \n",
    "\n",
    "The second decrease in first dose vaccinations comes before September of 2021. Second doses are starting to plateau during this time as well. After September, the rate at which first doses decrease across provinces starts to decrease as well. This indicates thats people are slowly starting to need their first dose again. According to the case graph, cases are still decreasing. \n",
    "\n",
    "People may be leaning towards getting their first dose, but with cases declining the sentiment around getting vaccinated may turn more negative or apathetic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed10d2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# has there been a peak in cases reported? \n",
    "# Is this consistent overtime? \n",
    "cases_overtime = covid.groupby(['Province/State', 'Month'])[['Cases']].sum()\n",
    "cases_overtime = cases_overtime.reset_index(['Province/State','Month'])\n",
    "\n",
    "# Reset the Month column data type to DateTime\n",
    "cases_overtime['Month']=pd.PeriodIndex(cases_overtime['Month'], freq='M').to_timestamp()\n",
    "\n",
    "# View the data\n",
    "cases_overtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a965aad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise case data\n",
    "fig, ax = plt.subplots()\n",
    "ax.ticklabel_format(style='plain')\n",
    "\n",
    "sns.lineplot(x='Month', y = 'Cases', hue='Province/State', data=cases_overtime)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9b4b33-b9e7-4edb-8b31-be365e6729bf",
   "metadata": {},
   "source": [
    "***Notes and observations:***\n",
    "Your observations here. (Double click to edit)\n",
    "\n",
    "***Examples could include:***\n",
    "- Are there other trends in terms of recoveries or hospitalisations compared to other features that you found interesting and that may add value in terms of the decision making process?\n",
    "- Any other observations regarding the data?\n",
    "- Any suggestions for improvements and further analysis?\n",
    "- What would your future data requirements be?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f7ddfe",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8869036-4f54-44b1-861d-86becdcfcc2d",
   "metadata": {},
   "source": [
    "## 5) Assignment activity 5: External data \n",
    "In the next section, you were supplied with a sample file and the question was asked to determine whether there are additional `#tags` or keywords that could potentially provide insights into your COVID-19 analysis. While the sample set is limited, you were asked to review the provided file and demonstrate the typical steps and make recommendations regarding future use of similar data sets to provide richer insights.\n",
    "\n",
    "### 5.1) Report expectations:\n",
    "- Demonstrate basic ability to work with Twitter data.\n",
    "- Search for hash-tags or keywords.\n",
    "- Create DataFrames and visualisations.\n",
    "- Note your observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e7d4c3-8227-4160-b503-33758f795a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the tweet data set\n",
    "import pandas as pd\n",
    "tweets = pd.read_csv('tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a907280f-5bce-4c51-bc5e-f0586bad8b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the data: info(), head()\n",
    "print(tweets.info())\n",
    "print()\n",
    "\n",
    "print(tweets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f659ecc8-b884-457f-8c26-e3d5e43bd807",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Explore the structure, count the tweets, get the elements of interest\n",
    "\n",
    "# Identify rows with null value\n",
    "tweets_null = tweets[tweets.isna().any(axis=1)]\n",
    "tweets_null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2cf736",
   "metadata": {},
   "source": [
    "### Look for Hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc53faf-5f04-46f8-897a-0bb2c16a0ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with the text only\n",
    "text = []\n",
    "rows = tweets.shape[0]\n",
    "#print(rows)\n",
    "for count in range(0, rows):\n",
    "    \n",
    "    result = tweets['text'].loc[count]\n",
    "    text.append(result)\n",
    "    count += 1\n",
    "\n",
    "print(type(text))\n",
    "tweets_text = pd.DataFrame(text)\n",
    "tweets_text\n",
    "\n",
    "# check for null values\n",
    "tweets_text.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83f6ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop null values\n",
    "tweets_text = tweets_text.dropna()\n",
    "tweets_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ce2d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Norah's way\n",
    "tweets['text'] = tweets['text'].astype(str)\n",
    "tweets_text = tweets['text'].apply(lambda x: x if x.strip() != None else None)\n",
    "tweets_text.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2e6eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the hashtags\n",
    "tags = []\n",
    "\n",
    "for y in [x.split(' ') for x in tweets_text.values]:\n",
    "    for z in y:\n",
    "        if '#' in z:\n",
    "            tags.append(z)\n",
    "            \n",
    "            \n",
    "# Create a series containing a count of our values\n",
    "tags = pd.Series(tags).value_counts()\n",
    "tags.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486c422b-4789-41c6-80f3-0edd28e51493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to a DataFrame\n",
    "tweets_tags = pd.DataFrame(tags)\n",
    "tweets_tags.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce5d278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index and rename the columns\n",
    "tweets_tags = tags.reset_index()\n",
    "\n",
    "# Rename the columns\n",
    "tweets_tags.columns = ['Hashtag', 'Count']\n",
    "\n",
    "# Sense check the data\n",
    "tweets_tags.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69812d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types\n",
    "print(tweets_tags.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5084682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display hashtags that appeared more than 100 times\n",
    "display(tweets_tags.loc[(tweets_tags['Count'] > 100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247178c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the hashtag counts\n",
    "import seaborn as sns\n",
    "sns.barplot(x='Count', y='Hashtag', data=tweets_tags.loc[(tweets_tags['Count'] > 100)]).set(title='Popular Hashtags')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb26e2f",
   "metadata": {},
   "source": [
    "### Look for common keywords associated with the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24f4fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_text.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d608c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# develop a common words library\n",
    "common = ['the', 'and', 'it', 'by', 'new', 'my', 'read', \"we're\", 'need', 'us', 'they',\n",
    "         \"they're\", 'me', 'people', 'with', 'those', 'told', 'said', 'links', 'to', \n",
    "         'go', 'going', 'is', 'not', ' which', 'many', \"can't\", 'on', 'behind', 'and', \n",
    "         'much', 'but', 'a', 'lot', 'where', 'you', 'of', 'in', 'for', 'from', 'are', 'that'\n",
    "          '&amp', 'The', '-', 'at', 'I', 'have', 'as', 'this', 'be', 'has', 'was', 'more', 'all', 'or',\n",
    "          'we', 'will', 'can', 'an', 'been', 'about', 'our', 'your', 'now', 'their', 'over', 'after', \n",
    "          'cause', 'get', 'still', 'just', 'please', 'Please', 'than', 'a', 'do', 'since', \n",
    "          'into', 'what', '@', 'there','https', 'any', 'It', '\\n', 'he', 'her', 'say', 'As', \n",
    "          \"it's\", 'want', 'keep', \" \", '&amp;','that', 'who', 'its', 'being', 'up', 'like', '|', 'up', 'like', \"I'm\", 'how', 'cause', \n",
    "          'get', 'via', 'first', 'had', 'no', 'may', 'way', 'were','out', 'per', \"I'm\", 'safe', 'so',\n",
    "         '-', 'during', 'A', 'For', 'if', 'one']\n",
    "\n",
    "common_words = pd.Series(common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bab2a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the tweets DataFrame into a list of tweets\n",
    "tweets_text_values = tweets_text.values\n",
    "tweets_text_values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f308385a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip out the words in the text which are not in the library above\n",
    "\n",
    "# Create a list of words\n",
    "t = [y.split(' ') for y in tweets_text_values]\n",
    "\n",
    "# Flatten the list\n",
    "import re\n",
    "tweets_words_split = []\n",
    "for i in t:\n",
    "    for j in i: \n",
    "        if j.find(',') != -1:\n",
    "            j = re.sub(\",\",\"\", j)\n",
    "            tweets_words_split.append(j)\n",
    "        else:\n",
    "            tweets_words_split.append(j)\n",
    "\n",
    "# Remove https and common words\n",
    "words = []\n",
    "for x in tweets_words_split:\n",
    "    if x not in common: \n",
    "        if x.find('https') != -1 or not x.startswith('#') :\n",
    "            words.append(x)\n",
    "            \n",
    "# Create a series containing the counts of common words\n",
    "words_S = pd.Series(words).value_counts()\n",
    "words_S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f9fde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame\n",
    "words_df = pd.DataFrame(words_S)\n",
    "\n",
    "words_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a806f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the words index\n",
    "words_df = words_df.reset_index()\n",
    "words_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d3c25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the column name for ease\n",
    "words_df.columns = ['Word', 'Count']\n",
    "words_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7ab459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose words with 200 or more uses\n",
    "display(words_df.loc[(words_df['Count'] > 90)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cba53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the common key words\n",
    "sns.barplot(x='Count', y='Word', data=words_df.loc[(words_df['Count'] > 90)]).set(title='Popular Common Words')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8333499",
   "metadata": {},
   "source": [
    "I chose to look at words with 90 or more mentions. It it evident that something happened in Greece at the time of the data collection. \n",
    "\n",
    "Other interesting common word associations were Boris Johnson, 'protection', 'deaths'and 'signs'. \n",
    "\n",
    "While this does not say too much about what people's sentiment towards the Covid Vaccine is, it can be noted that people are talking about the vaccine and Covid-19. The government can use this to their advantage and promote their vaccination strategy online. \n",
    "\n",
    "Using words such as deaths or Deaths, cases and vaccine would be able to build traction online when the campaign is actually implemented. \n",
    "\n",
    "The best hashtags to use are \"Covid19\" and \"CovidIsNotOver\". This corresponds perfectly with what the recoveries, hospitalizations and deaths data indicates: that a peak in cases is over. People may be becoming more relaxed with restictions and cases may begin to rise again. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0c7137-06dc-4cea-bc3a-5bedaeb8af86",
   "metadata": {},
   "source": [
    "### 5.2) Presentation expectations:\n",
    "Discuss whether external data could potentially be used and whether it is a viable solution to pursue. Discuss your assumptions and suggestions. \n",
    "\n",
    "Points to consider:\n",
    "- What insights can be gained from the data?\n",
    "- What are the advantages and disadvantages of using external data?\n",
    "- How would you suggest using external data in the project?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8926db",
   "metadata": {},
   "source": [
    "External data, when scraped from the internet, is cheap to aquire. It enables data from different sources to be used in one project. This enables different opinions or view point to be included in the analysis which may not have been included using data provided from the client. \n",
    "\n",
    "External data must be used ethically, and there can be gaps between what is legal and what is ethical. It may also be erroneous and introduce incorrect data into the analysis. Data scraped from websites like Twitter is not owned by the analyst but licensed by the company. This limits how you can use the data. \n",
    "\n",
    "Scraped data also has to be updated frequently to ensure it is accurate. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f28239b",
   "metadata": {},
   "source": [
    "In this project, I would use the data to try understand what people are talking about in association to Covid. This can give insight into what hashtags to use and what \"buzz words\" will inform the campaign marketing materials."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8339200b-2081-47dc-96f0-d62a408f4d35",
   "metadata": {},
   "source": [
    "## 6) Assignment activity 6: \n",
    "\n",
    "### 6.1) Report expectations:\n",
    "- Demonstrate using external function and interpret results.\n",
    "- Note observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a64a2f-2417-4dc2-8e2e-c99607b03dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can copy and paste the relevant code cells from the provided template here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6cc49f-48f6-4388-8adb-ef12d8eadf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and select relevant subset of the data\n",
    "# Make sure to change the relative path to function in your environment\n",
    "ds1 = pd.read_csv('covid_19_uk_cases.csv')\n",
    "ds2 = pd.read_csv('covid_19_uk_vaccinated.csv')\n",
    "\n",
    "sample = ds1[['Province/State','Date','Hospitalised']]\n",
    "\n",
    "# Sense check the data\n",
    "print(sample.head)\n",
    "print(sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ac29ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select data for the Ilse of Man\n",
    "sample_ci = sample[sample['Province/State'] == \"Isle of Man\"]\n",
    "\n",
    "# Sense check the data\n",
    "print(sample_ci.head())\n",
    "print(sample_ci.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596125f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to plot moving averages\n",
    "def plot_moving_average(series, window, plot_intervals=False, scale=1.96):\n",
    "    \n",
    "    # Create a rolling window to calculate the rolling mean using the series.rolling function\n",
    "    rolling_mean = series.rolling(window=window).mean()\n",
    "    \n",
    "    # Declare the dimensions for the plot, plot name and plot the data consisting of the rolling mean from above \n",
    "    plt.figure(figsize=(18,4))\n",
    "    plt.title('Moving average\\n window size = {}'.format(window))\n",
    "    plt.plot(rolling_mean, 'g', label='Simple moving average trend')\n",
    "\n",
    "    \n",
    "    # Plot confidence intervals for smoothed values\n",
    "    if plot_intervals:\n",
    "        \n",
    "        # Calculate the mean absolute square \n",
    "        mae = mean_absolute_error(series[window:], rolling_mean[window:])\n",
    "        \n",
    "        # Calculate the standard deviation using numpy's std function\n",
    "        deviation = np.std(series[window:] - rolling_mean[window:])\n",
    "        \n",
    "        # Calculate the upper and lower bounds \n",
    "        lower_bound = rolling_mean - (mae + scale * deviation)\n",
    "        upper_bound = rolling_mean + (mae + scale * deviation)\n",
    "        \n",
    "        # Name and style upper and lower bound lines and labels \n",
    "        plt.plot(upper_bound, 'r--', label='Upper bound / Lower bound')\n",
    "        plt.plot(lower_bound, 'r--')\n",
    "    \n",
    "    # Plot the actual values for the entire timeframe\n",
    "    plt.plot(series[window:], label='Actual values')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc65efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to calculate the mean absolute error\n",
    "def mean_absolute_error(a, b): return abs(b - a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1fbcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the use of the function to plot moving averages\n",
    "\n",
    "# Convert the dates in the sample_ci data to datetime \n",
    "sample_ci['Date'] = pd.to_datetime(sample_ci['Date'])\n",
    "print(sample_ci.dtypes)\n",
    "\n",
    "# Set the date as the index\n",
    "sample_ci = sample_ci.set_index('Date')\n",
    "\n",
    "# Plot the moving averages\n",
    "plot_moving_average(sample_ci['Hospitalised'], 7, plot_intervals=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72f711e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the top three days with biggest difference between daily value and rolling 7-day mean\n",
    "s = sample_ci.copy()\n",
    "s_rolling = s['Hospitalised'].rolling(window=7).mean()\n",
    "s['error'] = mean_absolute_error(s['Hospitalised'][7:], s_rolling[7:])\n",
    "print(s.sort_values('error', ascending=False).head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6eabcd",
   "metadata": {},
   "source": [
    "Mean absolute error represents the distance between the rolling 7 day average for a data point and the true value of the data point itself.  It is often used as a measure of the accuracy of the model. \n",
    "\n",
    "The above code block shows the largest differences between the moving average estimate of a data point and the true value. This is misleading because Mean Absolute Error is actually calculated by aggregating the absolute error terms to give an indication of how well a model performs on average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f94dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average of the absolute error terms. \n",
    "s['error'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838de178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat for Turks and Caicos\n",
    "sample_ci2 = sample[sample['Province/State'] == \"Turks and Caicos Islands\"]\n",
    "\n",
    "# Convert the dates in the sample_ci data to datetime \n",
    "sample_ci2['Date'] = pd.to_datetime(sample_ci2['Date'])\n",
    "print(sample_ci2.dtypes)\n",
    "\n",
    "# Set the date as the index\n",
    "sample_ci2 = sample_ci2.set_index('Date')\n",
    "\n",
    "# Sense check the data\n",
    "print(sample_ci2.head())\n",
    "print(sample_ci2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29acfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the moving averages\n",
    "plot_moving_average(sample_ci2['Hospitalised'], 7, plot_intervals=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc46de79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat for British Virgin Islands\n",
    "sample_ci3 = sample[sample['Province/State'] == \"British Virgin Islands\"]\n",
    "\n",
    "# Convert the dates in the sample_ci data to datetime \n",
    "sample_ci3['Date'] = pd.to_datetime(sample_ci3['Date'])\n",
    "print(sample_ci3.dtypes)\n",
    "\n",
    "# Set the date as the index\n",
    "sample_ci3 = sample_ci3.set_index('Date')\n",
    "\n",
    "# Sense check the data\n",
    "print(sample_ci3.head())\n",
    "print(sample_ci3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f5fad2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the moving averages\n",
    "plot_moving_average(sample_ci2['Hospitalised'], 7, plot_intervals=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbe4575",
   "metadata": {},
   "source": [
    "The above places of interest have been indentified as suitable targets for the government's campaign. Below the Channel Islands are checked as they were treated as outlying data during the analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758934e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place of interest: Channel Islands\n",
    "sample_ci4 = sample[sample['Province/State'] == \"Channel Islands\"]\n",
    "\n",
    "# Convert the dates in the sample_ci data to datetime \n",
    "sample_ci4['Date'] = pd.to_datetime(sample_ci4['Date'])\n",
    "print(sample_ci4.dtypes)\n",
    "\n",
    "# Set the date as the index\n",
    "sample_ci4 = sample_ci4.set_index('Date')\n",
    "\n",
    "# Sense check the data\n",
    "print(sample_ci4.head())\n",
    "print(sample_ci4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5c8b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the moving averages\n",
    "plot_moving_average(sample_ci2['Hospitalised'], 7, plot_intervals=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3009898f",
   "metadata": {},
   "source": [
    "The Channel Islands follow the same pattern as the other three Provinces. I conclude that hospitalisations reached a peak after January of 2021, ending during February."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220b2cec-b7d0-4da5-84db-c7e4b82dd305",
   "metadata": {},
   "source": [
    "### 6.2) Presentation expectations:\n",
    "- **Question 1**: We have heard of both qualitative and quantitative data from the previous consultant. What are the differences between the two? Should we use only one or both of these types of data and why? How can these be used in business predictions? Could you provide examples of each?\n",
    "- **Question 2**: We have also heard a bit about the need for continuous improvement. Why should this be implemented, it seems like a waste of time. Why cant we just implement the current project as it stands and move on to other pressing matters?\n",
    "- **Question 3**: As a government, we adhere to all data protection requirements and have good governance in place. We only work with aggregated data and therefore will not expose any personal details. Have we covered everything from a data ethics standpoint? Is there anything else we need to implement from a data ethics perspective? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25506de6",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "- What is the difference between qualitative and quantitative data? How can these be used in business predictions?\n",
    "\n",
    "Qualitative data is data is defined by categories and descriptions. The categories can have a distinct ordering, such as distinction levels for marks, but do not have to. The data values are assigned to categories based on the observation's characteristics. Participant's in a study on hair health may be classified by hair colour, for example. \n",
    "\n",
    "Quantitative data is numerical data that can be represented using an ordinal, interval or ratio scale. Observations can be discrete (meaning they can take on only fixed values in an interval) or continuous (meaning they can have take on any value along an interval). The weight of babies is a continuous, quantitative variable.\n",
    "\n",
    "Quantitative data can be to forecast trends, determine the correlation between variables and predict future events. These statistical methods can be applied in business to better understand the relationships between customer behaviour and buying patterns, predict what effect changes in input prices will have on operations and plan for future stock. This is only a few of the ways quantitative data can be used to drive decision making. \n",
    "\n",
    "Qualitative data is used to provide insights for short-term decision making. This is can be in the form of an opinion poll on facebook to check which product clients would purchase, or measure customer sentiment towards an idea."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82723100",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "Continuous improvement ensures that your processes remain efficient and accurate. In using data from the web, continuously updating your information ensures the project produces the most accurate results. In the same vein, there are always faster and improved ways of coding and proccessing information being introduced. Keeping up with technology ensures the longevity of your project and will save time, money and effort in the long run. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9858e0f",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f63c43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
